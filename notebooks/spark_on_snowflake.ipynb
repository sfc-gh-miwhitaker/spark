{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Your Spark Code on Snowflake\n",
        "\n",
        "**For Spark/Databricks users:** This notebook shows your existing PySpark code running on Snowflake - unchanged.\n",
        "\n",
        "No clusters to manage. No data movement. Just your code, running on Snowflake's engine.\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll See\n",
        "\n",
        "1. Standard PySpark DataFrame operations (the code you already write)\n",
        "2. Running on Snowflake via Snowpark Connect\n",
        "3. Results from Snowflake's compute engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Connect to Snowflake\n",
        "\n",
        "This creates a SparkSession that runs on Snowflake instead of a local cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Snowpark Connect - this replaces your SparkSession.builder code\n",
        "from snowflake import snowpark_connect\n",
        "\n",
        "spark = snowpark_connect.server.init_spark_session()\n",
        "print(f\"Connected! Spark API version: {spark.version}\")\n",
        "print(\"Execution engine: Snowflake\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Your Familiar PySpark Code\n",
        "\n",
        "Everything below is standard PySpark - the same code you'd write for Databricks or any Spark cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a DataFrame\n",
        "\n",
        "Just like you always do - `spark.createDataFrame()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample sales data - standard PySpark\n",
        "from pyspark.sql import Row\n",
        "\n",
        "sales_data = [\n",
        "    Row(order_id=1, customer=\"Alice\", product=\"Laptop\", amount=1200.00, region=\"West\"),\n",
        "    Row(order_id=2, customer=\"Bob\", product=\"Mouse\", amount=25.00, region=\"East\"),\n",
        "    Row(order_id=3, customer=\"Alice\", product=\"Keyboard\", amount=75.00, region=\"West\"),\n",
        "    Row(order_id=4, customer=\"Charlie\", product=\"Monitor\", amount=350.00, region=\"East\"),\n",
        "    Row(order_id=5, customer=\"Alice\", product=\"Webcam\", amount=89.00, region=\"West\"),\n",
        "    Row(order_id=6, customer=\"Bob\", product=\"Laptop\", amount=1100.00, region=\"East\"),\n",
        "    Row(order_id=7, customer=\"Diana\", product=\"Mouse\", amount=25.00, region=\"Central\"),\n",
        "    Row(order_id=8, customer=\"Charlie\", product=\"Keyboard\", amount=80.00, region=\"East\"),\n",
        "    Row(order_id=9, customer=\"Diana\", product=\"Monitor\", amount=400.00, region=\"Central\"),\n",
        "    Row(order_id=10, customer=\"Eve\", product=\"Laptop\", amount=1300.00, region=\"West\"),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(sales_data)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter and Select\n",
        "\n",
        "Standard DataFrame operations - `.filter()`, `.select()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter for high-value orders\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "high_value = (\n",
        "    df\n",
        "    .filter(F.col(\"amount\") > 100)\n",
        "    .select(\"customer\", \"product\", \"amount\")\n",
        ")\n",
        "\n",
        "high_value.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aggregations\n",
        "\n",
        "`.groupBy()` and `.agg()` - exactly like you'd write them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer lifetime value\n",
        "customer_summary = (\n",
        "    df\n",
        "    .groupBy(\"customer\")\n",
        "    .agg(\n",
        "        F.count(\"order_id\").alias(\"total_orders\"),\n",
        "        F.sum(\"amount\").alias(\"total_spent\"),\n",
        "        F.avg(\"amount\").alias(\"avg_order\")\n",
        "    )\n",
        "    .orderBy(F.desc(\"total_spent\"))\n",
        ")\n",
        "\n",
        "customer_summary.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sales by region\n",
        "region_summary = (\n",
        "    df\n",
        "    .groupBy(\"region\")\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"orders\"),\n",
        "        F.sum(\"amount\").alias(\"revenue\")\n",
        "    )\n",
        "    .orderBy(F.desc(\"revenue\"))\n",
        ")\n",
        "\n",
        "region_summary.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Window Functions\n",
        "\n",
        "Running totals, rankings - the patterns you use for analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Window function: running total per customer\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "customer_window = Window.partitionBy(\"customer\").orderBy(\"order_id\")\n",
        "\n",
        "with_running_total = (\n",
        "    df\n",
        "    .withColumn(\"running_total\", F.sum(\"amount\").over(customer_window))\n",
        "    .withColumn(\"order_rank\", F.row_number().over(customer_window))\n",
        ")\n",
        "\n",
        "with_running_total.filter(F.col(\"customer\") == \"Alice\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spark SQL\n",
        "\n",
        "Register as a temp view and query with SQL - works exactly the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register as temp view\n",
        "df.createOrReplaceTempView(\"sales\")\n",
        "\n",
        "# Query with Spark SQL\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        product,\n",
        "        COUNT(*) as times_sold,\n",
        "        SUM(amount) as total_revenue,\n",
        "        AVG(amount) as avg_price\n",
        "    FROM sales\n",
        "    GROUP BY product\n",
        "    ORDER BY total_revenue DESC\n",
        "\"\"\")\n",
        "\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write to Snowflake Table\n",
        "\n",
        "Save your results directly to Snowflake - no data movement needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write aggregated results to a Snowflake table\n",
        "customer_summary.write.mode(\"overwrite\").saveAsTable(\"CUSTOMER_SUMMARY\")\n",
        "\n",
        "# Verify it's there\n",
        "spark.sql(\"SELECT * FROM CUSTOMER_SUMMARY\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## What Just Happened?\n",
        "\n",
        "All that PySpark code you just ran? **It executed on Snowflake's compute engine.**\n",
        "\n",
        "| What You Wrote | Where It Ran |\n",
        "|----------------|--------------|\n",
        "| `spark.createDataFrame()` | Snowflake |\n",
        "| `.filter()`, `.select()` | Snowflake |\n",
        "| `.groupBy().agg()` | Snowflake |\n",
        "| Window functions | Snowflake |\n",
        "| `spark.sql()` | Snowflake |\n",
        "| `.write.saveAsTable()` | Snowflake |\n",
        "\n",
        "### What You Didn't Have To Do\n",
        "\n",
        "- Spin up a Spark cluster\n",
        "- Configure executors and memory\n",
        "- Move data from Snowflake to Spark\n",
        "- Move results back to Snowflake\n",
        "- Manage Spark versions and dependencies\n",
        "\n",
        "### What Changed In Your Code?\n",
        "\n",
        "Just the session initialization:\n",
        "\n",
        "```python\n",
        "# Before (Databricks/Spark)\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "\n",
        "# After (Snowflake)\n",
        "from snowflake import snowpark_connect\n",
        "spark = snowpark_connect.server.init_spark_session()\n",
        "```\n",
        "\n",
        "**Everything else stays exactly the same.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Try your own code** - Copy a PySpark notebook you have and change only the session initialization\n",
        "2. **Check compatibility** - [Snowpark Connect Compatibility Guide](https://docs.snowflake.com/en/developer-guide/snowpark-connect/compatibility)\n",
        "3. **Assess your codebase** - [Snowpark Migration Accelerator](https://docs.snowflake.com/en/developer-guide/snowpark-migration-accelerator) analyzes your Spark code for compatibility"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
