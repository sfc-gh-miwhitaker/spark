{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PySpark on Snowflake\n",
    "\n",
    "Your PySpark code. Snowflake's compute. One line change.\n",
    "\n",
    "---\n",
    "\n",
    "**What this notebook demonstrates:**\n",
    "\n",
    "1. Standard PySpark DataFrame operations running on Snowflake via Snowpark Connect\n",
    "2. The only code change required: session initialization\n",
    "3. Direct read/write to Snowflake tables - no data movement"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "After importing this notebook, add the following package from the **Packages** menu:\n",
    "\n",
    "- `snowpark_connect`\n",
    "\n",
    "See [Import Python packages](https://docs.snowflake.com/en/user-guide/ui-snowsight/notebooks-import-packages) for details."
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup"
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake import snowpark_connect\n",
    "\n",
    "spark = snowpark_connect.server.init_spark_session()\n",
    "\n",
    "# Import PySpark AFTER initializing the session\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PySpark Operations\n",
    "\n",
    "Standard DataFrame API - nothing Snowflake-specific below."
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data = [\n",
    "    Row(order_id=1, customer=\"Alice\", product=\"Laptop\", amount=1200.00, region=\"West\"),\n",
    "    Row(order_id=2, customer=\"Bob\", product=\"Mouse\", amount=25.00, region=\"East\"),\n",
    "    Row(order_id=3, customer=\"Alice\", product=\"Keyboard\", amount=75.00, region=\"West\"),\n",
    "    Row(order_id=4, customer=\"Charlie\", product=\"Monitor\", amount=350.00, region=\"East\"),\n",
    "    Row(order_id=5, customer=\"Alice\", product=\"Webcam\", amount=89.00, region=\"West\"),\n",
    "    Row(order_id=6, customer=\"Bob\", product=\"Laptop\", amount=1100.00, region=\"East\"),\n",
    "    Row(order_id=7, customer=\"Diana\", product=\"Mouse\", amount=25.00, region=\"Central\"),\n",
    "    Row(order_id=8, customer=\"Charlie\", product=\"Keyboard\", amount=80.00, region=\"East\"),\n",
    "    Row(order_id=9, customer=\"Diana\", product=\"Monitor\", amount=400.00, region=\"Central\"),\n",
    "    Row(order_id=10, customer=\"Eve\", product=\"Laptop\", amount=1300.00, region=\"West\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(sales_data)\n",
    "df.show()"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Select"
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value = (\n",
    "    df\n",
    "    .filter(F.col(\"amount\") > 100)\n",
    "    .select(\"customer\", \"product\", \"amount\")\n",
    ")\n",
    "\n",
    "high_value.show()"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_summary = (\n",
    "    df\n",
    "    .groupBy(\"customer\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"amount\").alias(\"total_spent\"),\n",
    "        F.avg(\"amount\").alias(\"avg_order\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_spent\"))\n",
    ")\n",
    "\n",
    "customer_summary.show()"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_summary = (\n",
    "    df\n",
    "    .groupBy(\"region\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"orders\"),\n",
    "        F.sum(\"amount\").alias(\"revenue\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"revenue\"))\n",
    ")\n",
    "\n",
    "region_summary.show()"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_window = Window.partitionBy(\"customer\").orderBy(\"order_id\")\n",
    "\n",
    "with_running_total = (\n",
    "    df\n",
    "    .withColumn(\"running_total\", F.sum(\"amount\").over(customer_window))\n",
    "    .withColumn(\"order_rank\", F.row_number().over(customer_window))\n",
    ")\n",
    "\n",
    "with_running_total.filter(F.col(\"customer\") == \"Alice\").show()"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product,\n",
    "        COUNT(*) as times_sold,\n",
    "        SUM(amount) as total_revenue,\n",
    "        AVG(amount) as avg_price\n",
    "    FROM sales\n",
    "    GROUP BY product\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Table"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_summary.write.mode(\"overwrite\").saveAsTable(\"CUSTOMER_SUMMARY\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM CUSTOMER_SUMMARY\").show()"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Every operation above executed on Snowflake's compute engine. The DataFrame API calls were translated to Snowflake SQL and run on a Snowflake warehouse.\n",
    "\n",
    "**What you didn't need:**\n",
    "- Spark cluster provisioning or tuning\n",
    "- Data connectors or ETL to move data\n",
    "- Separate infrastructure to manage\n",
    "\n",
    "**What's different:** One import, one session initializer. That's the entire migration."
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Try your own code** - Swap the session initializer in an existing notebook\n",
    "- **Check compatibility** - [Snowpark Connect API Coverage](https://docs.snowflake.com/en/developer-guide/snowpark-connect/compatibility)\n",
    "- **Assess at scale** - [Snowpark Migration Accelerator](https://docs.snowflake.com/en/developer-guide/snowpark-migration-accelerator) scans your codebase for compatibility"
   ],
   "id": "cell-19"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}