{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark with Iceberg Tables on Snowflake\n",
    "\n",
    "Read and write Iceberg tables using your existing PySpark code.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need an **external volume** configured in Snowflake. This requires:\n",
    "\n",
    "1. **Cloud storage bucket** (S3, GCS, or Azure Blob)\n",
    "2. **IAM role** with read/write access to the bucket\n",
    "3. **External volume** in Snowflake pointing to that storage\n",
    "\n",
    "If you haven't set this up yet, see: [Configure an external volume for Iceberg tables](https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume)\n",
    "\n",
    "---\n",
    "\n",
    "**What this notebook demonstrates:**\n",
    "\n",
    "1. Creating Iceberg tables in Snowflake\n",
    "2. Reading Iceberg tables with PySpark DataFrame API\n",
    "3. Transforming data and writing results back to Iceberg"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these values for your environment."
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Update these values for your environment\n",
    "# =============================================================================\n",
    "\n",
    "DATABASE = \"SNOWFLAKE_EXAMPLE\"              # Database to use\n",
    "EXTERNAL_VOLUME = \"sfe_iceberg_demo_vol\"    # Your external volume name\n",
    "DEMO_SCHEMA = \"ICEBERG_SPARK_DEMO\"          # Schema to create for this demo\n",
    "BASE_PATH = \"spark_demo\"                    # Base path within your external volume"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Initialize Snowpark Connect and validate the configuration."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "from snowflake import snowpark_connect\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = snowpark_connect.server.init_spark_session()\n",
    "\n",
    "# Get Snowpark session for SQL DDL operations\n",
    "session = get_active_session()\n",
    "\n",
    "# Import PySpark AFTER initializing the session\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Validate external volume exists\n",
    "try:\n",
    "    session.sql(f\"DESC EXTERNAL VOLUME {EXTERNAL_VOLUME}\").collect()\n",
    "    print(f\"External volume '{EXTERNAL_VOLUME}' found\")\n",
    "except:\n",
    "    raise ValueError(\n",
    "        f\"\\n\\nExternal volume '{EXTERNAL_VOLUME}' not found.\\n\"\n",
    "        f\"Update the EXTERNAL_VOLUME variable in the Configuration cell above.\\n\"\n",
    "        f\"Run: SHOW EXTERNAL VOLUMES to see available volumes.\"\n",
    "    )\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create Iceberg Tables\n",
    "\n",
    "First, we'll create Iceberg tables from Snowflake's sample TPC-H data. This uses SQL via the Snowpark session."
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Schema for Iceberg"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the configured database and create schema for this demo\n",
    "session.sql(f\"USE DATABASE {DATABASE}\").collect()\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {DEMO_SCHEMA}\").collect()\n",
    "session.sql(f\"USE SCHEMA {DEMO_SCHEMA}\").collect()\n",
    "\n",
    "# Set the Iceberg catalog and external volume for the schema\n",
    "session.sql(f\"ALTER SCHEMA {DEMO_SCHEMA} SET CATALOG = 'SNOWFLAKE'\").collect()\n",
    "session.sql(f\"ALTER SCHEMA {DEMO_SCHEMA} SET EXTERNAL_VOLUME = '{EXTERNAL_VOLUME}'\").collect()\n",
    "\n",
    "print(f\"Schema '{DATABASE}.{DEMO_SCHEMA}' configured with external volume '{EXTERNAL_VOLUME}'\")"
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Iceberg Tables from Sample Data"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customers Iceberg table from TPC-H sample data\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE ICEBERG TABLE CUSTOMERS\n",
    "    BASE_LOCATION = '{BASE_PATH}/customers'\n",
    "    AS SELECT \n",
    "        C_CUSTKEY as customer_id,\n",
    "        C_NAME as name,\n",
    "        C_MKTSEGMENT as market_segment,\n",
    "        C_NATIONKEY as nation_id,\n",
    "        C_ACCTBAL as account_balance\n",
    "    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.CUSTOMER\n",
    "    LIMIT 10000\n",
    "\"\"\").collect()\n",
    "\n",
    "# Create nations Iceberg table\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE ICEBERG TABLE NATIONS\n",
    "    BASE_LOCATION = '{BASE_PATH}/nations'\n",
    "    AS SELECT \n",
    "        N_NATIONKEY as nation_id,\n",
    "        N_NAME as nation_name,\n",
    "        N_REGIONKEY as region_id\n",
    "    FROM SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Iceberg tables created\")"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Read Iceberg Tables with PySpark\n",
    "\n",
    "Now the fun part - read those Iceberg tables using standard PySpark DataFrame API."
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Iceberg tables as Spark DataFrames\n",
    "customers = spark.table(\"CUSTOMERS\")\n",
    "nations = spark.table(\"NATIONS\")\n",
    "\n",
    "print(f\"Customers: {customers.count()} rows\")\n",
    "print(f\"Nations: {nations.count()} rows\")\n",
    "\n",
    "customers.show(5)"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nations.show()"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Transform Data\n",
    "\n",
    "Standard PySpark transformations - joins, aggregations, window functions."
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join customers with nations\n",
    "# Note: Snowflake stores column names in uppercase, so use F.col() with uppercase names\n",
    "customers_enriched = (\n",
    "    customers\n",
    "    .join(nations, F.col(\"CUSTOMERS.NATION_ID\") == F.col(\"NATIONS.NATION_ID\"), \"left\")\n",
    "    .select(\n",
    "        F.col(\"CUSTOMERS.CUSTOMER_ID\").alias(\"customer_id\"),\n",
    "        F.col(\"CUSTOMERS.NAME\").alias(\"name\"),\n",
    "        F.col(\"CUSTOMERS.MARKET_SEGMENT\").alias(\"market_segment\"),\n",
    "        F.col(\"CUSTOMERS.ACCOUNT_BALANCE\").alias(\"account_balance\"),\n",
    "        F.col(\"NATIONS.NATION_NAME\").alias(\"nation_name\")\n",
    "    )\n",
    ")\n",
    "\n",
    "customers_enriched.show(10)"
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by market segment and nation\n",
    "segment_analysis = (\n",
    "    customers_enriched\n",
    "    .groupBy(\"market_segment\", \"nation_name\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"customer_count\"),\n",
    "        F.sum(\"account_balance\").alias(\"total_balance\"),\n",
    "        F.avg(\"account_balance\").alias(\"avg_balance\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_balance\"))\n",
    ")\n",
    "\n",
    "segment_analysis.show(15)"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank customers within each market segment by account balance\n",
    "segment_window = Window.partitionBy(\"market_segment\").orderBy(F.desc(\"account_balance\"))\n",
    "\n",
    "top_customers = (\n",
    "    customers_enriched\n",
    "    .withColumn(\"rank_in_segment\", F.row_number().over(segment_window))\n",
    "    .filter(F.col(\"rank_in_segment\") <= 3)\n",
    "    .orderBy(\"market_segment\", \"rank_in_segment\")\n",
    ")\n",
    "\n",
    "top_customers.show(15)"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Write Results to Iceberg\n",
    "\n",
    "Save transformed data back to a new Iceberg table."
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write aggregated results to a new Iceberg table\n",
    "segment_analysis.write.mode(\"overwrite\").saveAsTable(\"SEGMENT_ANALYSIS\")\n",
    "\n",
    "# Verify the write\n",
    "spark.table(\"SEGMENT_ANALYSIS\").show(10)"
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "| Operation | API |\n",
    "|-----------|-----|\n",
    "| Read Iceberg table | `spark.table(\"TABLE_NAME\")` |\n",
    "| Join tables | `.join()` |\n",
    "| Aggregate | `.groupBy().agg()` |\n",
    "| Window functions | `Window.partitionBy().orderBy()` |\n",
    "| Write to Iceberg | `.write.mode(\"overwrite\").saveAsTable()` |\n",
    "\n",
    "All operations executed on Snowflake's compute engine, reading from and writing to Iceberg tables stored in your external volume."
   ],
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup (Optional)\n",
    "\n",
    "Run this cell to remove the demo tables and schema."
   ],
   "attachments": {}
  },
  {
   "cell_type": "code",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "# Uncomment to clean up\n",
    "# session.sql(\"DROP ICEBERG TABLE IF EXISTS CUSTOMERS\").collect()\n",
    "# session.sql(\"DROP ICEBERG TABLE IF EXISTS NATIONS\").collect()\n",
    "# session.sql(\"DROP ICEBERG TABLE IF EXISTS SEGMENT_ANALYSIS\").collect()\n",
    "# session.sql(f\"DROP SCHEMA IF EXISTS {DEMO_SCHEMA}\").collect()\n",
    "# print(\"Cleanup complete\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}